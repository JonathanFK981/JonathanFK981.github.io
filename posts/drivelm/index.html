<!DOCTYPE html>
<html lang="zh-Hans"
  x-data
  :class="$store.darkMode.class()"
  :data-theme="$store.darkMode.theme()">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DriveLM | dO.ob&#39;s Blog</title>

    

<link rel="canonical" href="http://localhost:1313/posts/drivelm/" />



<meta name="author" content="Author Name" />
<meta name="description" content="" />


<meta name="keywords" content="Tag1,Tag2">



<meta name="generator" content="Hugo 0.146.2">


<meta property="og:url" content="http://localhost:1313/posts/drivelm/">
  <meta property="og:site_name" content="dO.ob&#39;s Blog">
  <meta property="og:title" content="DriveLM">
  <meta property="og:locale" content="zh_Hans">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-26T19:54:26+08:00">
    <meta property="article:modified_time" content="2025-05-26T19:54:26+08:00">
    <meta property="article:tag" content="Tag1">
    <meta property="article:tag" content="Tag2">




  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="DriveLM">




<link rel="stylesheet" href="/css/output.css" />




<script>
  MathJax = {
    tex: {
      inlineMath: [['\\(', '\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 


    
<script>
  MathJax = {
    tex: {
      inlineMath: [['\\(', '\\)'], ['$', '$']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 


    


<style>
  pre {
    padding: 1em;
    overflow: auto;
  }
</style>









    

    <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3/dist/cdn.min.js" integrity="sha256-PtHu0lJIiSHfZeNj1nFd6wTX+Squ255SGZ/fc8seCtM=" crossorigin="anonymous"></script>
  </head>

  <body x-data="{
    flip: false,
  }">
    
    <div id="dream-global-bg"></div>

    
<nav class="mt-4 lg:mt-8 py-4">

  
  <div class="container flex justify-between max-w-[65ch] mx-auto px-4 md:px-0">
  
    <section class="flex items-center gap-4">
      <div class="avatar cursor-pointer hover:avatar-online" @click="flip = !flip" title="ç¿»è½¬ä¸€ä¸‹ï¼">
        <div class="h-10 rounded-full">
          <img src="/img/logo.jpg" alt="Jonathan&#39;s Blog" />
        </div>
      </div>

      
      <div>
        
        <a href="http://localhost:1313/" class="text-lg font-semibold cursor-pointer">
          Jonathan&#39;s Blog
        </a>
        
        
      </div>
      
    </section>

    
    

    <div class="dropdown dropdown-end sm:hidden">
      <div tabindex="0" role="button" class="btn btn-ghost btn-square" aria-label="Select an option">
        <ion-icon name="menu" class="text-2xl"></ion-icon>
      </div>
      <ul tabindex="0" class="dropdown-content menu w-36 bg-base-100 rounded-box z-1 shadow-md">
        







<li>
  <div role="link" tabindex="0" class="inline-flex items-center p-2 cursor-pointer" @click="flip = !flip" title="å…³äº">
    <ion-icon name="information-circle"></ion-icon>å…³äº</div>
</li>





















<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/posts" title="å½’æ¡£">
    <ion-icon name="archive"></ion-icon>
    å½’æ¡£
  </a>
</li>




<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/categories" title="æ‰€æœ‰åˆ†ç±»">
    <ion-icon name="grid"></ion-icon>
    æ‰€æœ‰åˆ†ç±»
  </a>
</li>




<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/tags" title="æ‰€æœ‰æ ‡ç­¾">
    <ion-icon name="pricetags"></ion-icon>
    æ‰€æœ‰æ ‡ç­¾
  </a>
</li>






      </ul>
    </div>
    <section class="hidden sm:flex sm:items-center sm:gap-2 md:gap-4">
      

      
      




<div role="link" tabindex="0" class="text-sm font-semibold cursor-pointer hover:underline" @click="flip = !flip" title="å…³äº">å…³äº</div>





      
      





      
      





      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/posts" title="å½’æ¡£">
  <ion-icon class="group-hover:text-primary-content" name="archive"></ion-icon>
</a>


      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/categories" title="æ‰€æœ‰åˆ†ç±»">
  <ion-icon class="group-hover:text-primary-content" name="grid"></ion-icon>
</a>


      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/tags" title="æ‰€æœ‰æ ‡ç­¾">
  <ion-icon class="group-hover:text-primary-content" name="pricetags"></ion-icon>
</a>


      

      

      
    </section>
  </div>
</nav>


    <div class="flip-container" :class="{ 'flip-it': flip }">
      <div class="flipper">
        <div class="front">
          <div class="container">
            
<div class="lg:grid lg:grid-cols-4 gap-4 mt-4 px-4">
  <div class="hidden lg:block">
    
  </div>

  <div class="lg:col-span-2">
    <article class="mx-auto prose prose-quoteless dark:prose-invert" id="dream-single-post-main" itemscope itemtype="http://schema.org/Article">
      
  <meta itemprop="name" content="DriveLM">
  <meta itemprop="datePublished" content="2025-05-26T19:54:26+08:00">
  <meta itemprop="dateModified" content="2025-05-26T19:54:26+08:00">
  <meta itemprop="wordCount" content="1378">
  <meta itemprop="keywords" content="Tag1,Tag2">

      <header>
        <h1 itemprop="headline">DriveLM</h1>
        <p class="text-sm">
          
            æ˜ŸæœŸä¸€, 5æœˆ 26, 2025
          

          | <span>7åˆ†é’Ÿé˜…è¯»</span>

          
          | <span>æ›´æ–°äº
            
              æ˜ŸæœŸä¸€, 5æœˆ 26, 2025
            </span>
          
        </p>

        
        <div class="flex justify-between">
          
            <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    
      <span itemprop="name">Author Name</span>
    
  
  </span>
</div>

          

          <div class="flex items-center gap-2">
  
  

  
  
  
  
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://x.com/intent/post?text=DriveLM&amp;url=http://localhost:1313/posts/drivelm/" target="_blank" rel="noopener noreferrer"
      title="Share on X">
      <ion-icon class="group-hover:text-primary-content" name="logo-x"></ion-icon>
    </a>
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://facebook.com/sharer/sharer.php?u=http://localhost:1313/posts/drivelm/" target="_blank" rel="noopener noreferrer"
      title="Share on Facebook">
      <ion-icon class="group-hover:text-primary-content" name="logo-facebook"></ion-icon>
    </a>
  
    <a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary"
      href="https://wa.me/?text=DriveLM%20http://localhost:1313/posts/drivelm/" target="_blank" rel="noopener noreferrer"
      title="Share on WhatsApp">
      <ion-icon class="group-hover:text-primary-content" name="logo-whatsapp"></ion-icon>
    </a>
  

  
  
</div>

        </div>
      </header>

      <section id="dream-single-post-content" itemprop="articleBody">
        
          <img class="w-full z-30" src="/img/a.jpg" alt="DriveLM" />
        

        <!-- Cut out summary from your post content here. -->
<!-- The remaining content of your post. -->
<p>Abstract.
We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction, and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen sensor configurations. Our question-wise ablation study shows that the performance gain comes from the rich annotation of prediction and planning QA pairs in the graph structure. All data, models, and an official evaluation server are available at <a href="https://github.com/OpenDriveLab/DriveLM" target="_blank">https://github.com/OpenDriveLab/DriveLM</a>
.</p>
<p>æ‘˜è¦
æˆ‘ä»¬ç ”ç©¶äº†åœ¨ç½‘ç»œè§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚ä½•é›†æˆåˆ°ç«¯åˆ°ç«¯é©¾é©¶ç³»ç»Ÿä¸­ï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›å¹¶å®ç°ä¸äººç±»ç”¨æˆ·çš„äº¤äº’ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•é€šè¿‡å•è½®è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä½¿è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”é©¾é©¶åœºæ™¯ï¼Œä½†äººç±»é©¾é©¶å‘˜ä¼šé€šè¿‡å¤šä¸ªæ­¥éª¤å¯¹å†³ç­–è¿›è¡Œæ¨ç†â€”â€”ä»å…³é”®ç‰©ä½“çš„å®šä½å¼€å§‹ï¼Œäººç±»ä¼šåœ¨é‡‡å–è¡ŒåŠ¨å‰è¯„ä¼°ç‰©ä½“é—´çš„ç›¸äº’ä½œç”¨ã€‚æ ¸å¿ƒè§‚ç‚¹åœ¨äºï¼Œæˆ‘ä»¬æå‡ºçš„å›¾è§†è§‰é—®ç­”ï¼ˆGraph VQAï¼‰ä»»åŠ¡é€šè¿‡æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’é—®ç­”å¯¹æ¥å»ºæ¨¡å›¾ç»“æ„æ¨ç†ï¼Œä»è€Œè·å¾—äº†ä¸€ä¸ªæ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹çš„åˆé€‚ä»£ç†ä»»åŠ¡ã€‚</p>
<p>æˆ‘ä»¬åŸºäºnuSceneså’ŒCARLAæ„å»ºäº†æ•°æ®é›†ï¼ˆDriveLM-Dataï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºçº¿æ–¹æ³•ï¼ˆDriveLM-Agentï¼‰æ¥è”åˆæ‰§è¡Œå›¾è§†è§‰é—®ç­”å’Œç«¯åˆ°ç«¯é©¾é©¶ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œå›¾è§†è§‰é—®ç­”ä¸ºé©¾é©¶åœºæ™¯æ¨ç†æä¾›äº†ä¸€ä¸ªç®€å•ä¸”æœ‰åŸåˆ™çš„æ¡†æ¶ï¼Œè€ŒDriveLM-Dataä¸ºè¯¥ä»»åŠ¡æä¾›äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚ä¸æœ€å…ˆè¿›çš„é©¾é©¶ä¸“ç”¨æ¶æ„ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„DriveLM-AgentåŸºçº¿åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ç«äº‰åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“åœ¨æœªè§ä¼ æ„Ÿå™¨é…ç½®ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼°æ—¶ï¼Œå…¶ä¼˜åŠ¿å°¤ä¸ºæ˜¾è‘—ã€‚æˆ‘ä»¬çš„é€é¡¹æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæ€§èƒ½æå‡æ¥è‡ªäºå›¾ç»“æ„ä¸­é¢„æµ‹å’Œè§„åˆ’é—®ç­”å¯¹çš„ä¸°å¯Œæ³¨é‡Šã€‚</p>
<p>æ‰€æœ‰æ•°æ®ã€æ¨¡å‹å’Œå®˜æ–¹è¯„ä¼°æœåŠ¡å™¨å‡å¯åœ¨https://github.com/OpenDriveLab/DriveLMè·å–ã€‚</p>
<p>1 ç»¼è¿°
Current Autonomous Driving (AD) stacks are still lacking crucial capabilities [10, 15]. One key requirement is generalization, which involves the ability to handle unseen scenarios or unfamiliar sensor configurations. A secondary requirement pertains to the interaction of these models with humans, highlighted, for example, by EU regulations that mandate explainability in deployment [3]. Furthermore, unlike today&rsquo;s AD models, humans do not navigate based on geometrically precise bird&rsquo;s-eye view (BEV) representations [17, 32, 46]. Instead, humans implicitly perform object-centric perception, prediction, and planning (which we refer to as P-3): a rough identification and localization of key objects, followed by reasoning about their possible movement and aggregation of this information into a driving action [60, 76].</p>
<p>å½“å‰çš„è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç³»ç»Ÿä»ç¼ºä¹å…³é”®èƒ½åŠ›[10,15]ã€‚å…¶ä¸­ä¸€é¡¹æ ¸å¿ƒéœ€æ±‚æ˜¯<strong>æ³›åŒ–èƒ½åŠ›</strong>ï¼Œå³å¤„ç†æœªè§åœºæ™¯æˆ–é™Œç”Ÿä¼ æ„Ÿå™¨é…ç½®çš„èƒ½åŠ›ã€‚ç¬¬äºŒé¡¹éœ€æ±‚æ¶‰åŠæ¨¡å‹ä¸äººç±»çš„äº¤äº’â€”â€”ä¾‹å¦‚ï¼Œæ¬§ç›Ÿæ³•è§„è¦æ±‚éƒ¨ç½²çš„æ¨¡å‹å…·å¤‡å¯è§£é‡Šæ€§[3]ï¼Œå³å‡¸æ˜¾äº†è¿™ä¸€ç‚¹ã€‚æ­¤å¤–ï¼Œä¸å½“ä»Šçš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹ä¸åŒï¼Œäººç±»å¹¶éåŸºäºå‡ ä½•ç²¾ç¡®çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰è¡¨å¾è¿›è¡Œå¯¼èˆª[17,32,46]ã€‚ç›¸åï¼Œäººç±»ä¼šéšæ€§åœ°æ‰§è¡Œ<strong>ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ï¼ˆç®€ç§°P-3ï¼‰</strong>ï¼šé¦–å…ˆå¯¹å…³é”®ç‰©ä½“è¿›è¡Œç²—ç•¥è¯†åˆ«å’Œå®šä½ï¼Œéšåæ¨ç†å…¶å¯èƒ½çš„è¿åŠ¨è½¨è¿¹ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯æ•´åˆä¸ºé©¾é©¶åŠ¨ä½œ[60,76]ã€‚</p>
<p>Simultaneously, another field has been forging ahead: Vision-Language Models (VLMs) [47, 54, 90, 106]. These models have several strengths. First, they hold a base understanding of the world from internet-scale data that could potentially facilitate generalization for planning in AD. In fact, this sort of generalization has already been achieved by VLMs for simpler robotics tasks [23, 108]. Second, the use of language representations as an input and output offers a platform for human-friendly interaction with these models, unlike bounding boxes or trajectories that are more common to current methods [19, 31, 49, 70]. Finally, VLMs are able to make decisions in multiple steps linked by logical reasoningsoning [5,21,93,95,103,108]. Importantly, even though they reason in multiple separate steps, VLMs are end-to-end differentiable architectures, a characteristic that is highly desirable for autonomous driving [10].</p>
<p>ä¸æ­¤åŒæ—¶ï¼Œå¦ä¸€ä¸ªé¢†åŸŸä¹Ÿåœ¨è“¬å‹ƒå‘å±•ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰[47, 54, 90, 106]ã€‚è¿™ç±»æ¨¡å‹å…·å¤‡å¤šé¡¹ä¼˜åŠ¿ï¼š<br>
é¦–å…ˆï¼Œå®ƒä»¬é€šè¿‡äº’è”ç½‘è§„æ¨¡çš„æ•°æ®å½¢æˆäº†å¯¹ä¸–ç•Œçš„åŸºç¡€ç†è§£ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºæå‡è‡ªåŠ¨é©¾é©¶è§„åˆ’çš„æ³›åŒ–èƒ½åŠ›ã€‚äº‹å®ä¸Šï¼ŒVLMså·²åœ¨æ›´ç®€å•çš„æœºå™¨äººä»»åŠ¡ä¸­å®ç°äº†è¿™ç§æ³›åŒ–èƒ½åŠ›[23, 108]ã€‚<br>
å…¶æ¬¡ï¼Œä¸å½“å‰æ–¹æ³•ä¸­å¸¸è§çš„è¾¹ç•Œæ¡†æˆ–è½¨è¿¹ä¸åŒï¼ŒVLMsä½¿ç”¨è¯­è¨€è¡¨å¾ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºï¼Œä¸ºæ¨¡å‹ä¸äººç±»çš„å‹å¥½äº¤äº’æä¾›äº†å¹³å°[19, 31, 49, 70]ã€‚<br>
æœ€åï¼ŒVLMsèƒ½å¤Ÿé€šè¿‡é€»è¾‘æ¨ç†å®ç°å¤šæ­¥éª¤å†³ç­–[5,21,93,95,103,108]ã€‚é‡è¦çš„æ˜¯ï¼Œå°½ç®¡VLMsé€šè¿‡å¤šä¸ªç‹¬ç«‹æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œä½†å®ƒä»¬å±äºç«¯åˆ°ç«¯å¯å¾®åˆ†æ¶æ„â€”â€”è¿™ä¸€ç‰¹æ€§æ­£æ˜¯è‡ªåŠ¨é©¾é©¶æ‰€é«˜åº¦éœ€è¦çš„[10]ã€‚</p>
<p>Recent work towards enabling the application of VLMs to AD systems falls into two categories: scene-level or single object-level Visual Question Answering (VQA). Scene-level VQA refers to the task of describing the driving behavior by one or two supporting reasons, e.g., &ldquo;The car is moving into the right lane because it is safe to do so.&rdquo; [41,42]. Single object-level VQA formulates the understanding of the ego vehicle&rsquo;s response to a single object by a chain of QAs in the form of &ldquo;what-which-where-how-why,&rdquo; e.g., &ldquo;The ego vehicle stops because there is a pedestrian in a white shirt crossing the intersection in front of the ego vehicle and it does not want to crash into the pedestrian.&rdquo; [56,67,71]. Unfortunately, neither of these paradigms provides a suitable proxy task to mimic the P-3 reasoning process in humans, who consider multiple objects and reason about each in multiple steps. Therefore, in this paper, we propose a new task, along with corresponding datasets and a baseline model architecture (Fig. 1).</p>
<p>è¿‘æœŸå°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç ”ç©¶å·¥ä½œä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåœºæ™¯çº§æˆ–å•ç‰©ä½“çº§çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€‚åœºæ™¯çº§VQAæŒ‡çš„æ˜¯é€šè¿‡ä¸€ä¸¤ä¸ªæ”¯æŒæ€§ç†ç”±æè¿°é©¾é©¶è¡Œä¸ºçš„ä»»åŠ¡ï¼Œä¾‹å¦‚â€œè½¦è¾†æ­£åœ¨é©¶å…¥å³è½¦é“ï¼Œå› ä¸ºè¿™æ ·åšæ˜¯å®‰å…¨çš„â€[41,42]ã€‚å•ç‰©ä½“çº§VQAåˆ™é€šè¿‡â€œä»€ä¹ˆ-å“ªä¸ª-å“ªé‡Œ-å¦‚ä½•-ä¸ºä»€ä¹ˆâ€çš„é—®ç­”é“¾æ¥æ„å»ºå¯¹è‡ªè½¦å“åº”å•ä¸ªç‰©ä½“çš„ç†è§£ï¼Œä¾‹å¦‚â€œè‡ªè½¦åœæ­¢æ˜¯å› ä¸ºæœ‰ä¸€ä¸ªç©¿ç™½è¡¬è¡«çš„è¡Œäººæ­£åœ¨è‡ªè½¦å‰æ–¹çš„è·¯å£æ¨ªç©¿ï¼Œä¸”è‡ªè½¦ä¸æƒ³æ’åˆ°è¡Œäººâ€[56,67,71]ã€‚é—æ†¾çš„æ˜¯ï¼Œè¿™ä¸¤ç§èŒƒå¼éƒ½æœªèƒ½æä¾›ä¸€ä¸ªåˆé€‚çš„ä»£ç†ä»»åŠ¡æ¥æ¨¡æ‹Ÿäººç±»çš„P-3æ¨ç†è¿‡ç¨‹â€”â€”äººç±»ä¼šè€ƒè™‘å¤šä¸ªç‰©ä½“å¹¶å¯¹æ¯ä¸ªç‰©ä½“è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œå¹¶é…å¥—ç›¸åº”çš„æ•°æ®é›†å’ŒåŸºçº¿æ¨¡å‹æ¶æ„ï¼ˆå›¾1ï¼‰ã€‚</p>
<p><strong>Task. Graph Visual Question Answering (GVQA)</strong> involves formulating P1-3 reasoning as a series of question-answer pairs (QAs) in a directed graph. Its key difference to the aforementioned VQA tasks for AD is the availability of logical dependencies between QAs which can be used to guide the answering process. GVQA also encompasses questions regarding behavior and motion planning, with dedicated metrics (details in Section 2).</p>
<p><strong>ä»»åŠ¡ï¼šå›¾è§†è§‰é—®ç­”ï¼ˆGVQAï¼‰</strong><br>
å›¾è§†è§‰é—®ç­”ï¼ˆGVQAï¼‰ä»»åŠ¡å°†P1-3æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºæœ‰å‘å›¾ä¸­çš„ä¸€ç³»åˆ—é—®ç­”å¯¹ï¼ˆQAsï¼‰ã€‚å…¶ä¸ä¸Šè¿°è‡ªåŠ¨é©¾é©¶VQAä»»åŠ¡çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼Œé—®ç­”å¯¹ä¹‹é—´å­˜åœ¨å¯ç”¨äºæŒ‡å¯¼å›ç­”è¿‡ç¨‹çš„é€»è¾‘ä¾èµ–å…³ç³»ã€‚GVQAè¿˜æ¶µç›–äº†ä¸è¡Œä¸ºå’Œè¿åŠ¨è§„åˆ’ç›¸å…³çš„é—®é¢˜ï¼Œå¹¶é…å¤‡äº†ä¸“é—¨çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå…·ä½“ç»†èŠ‚è§ç¬¬2èŠ‚ï¼‰ã€‚</p>
<p><strong>Datasets. DriveLM-nuScenes and DriveLM-CARLA</strong> consist of annotated QAs, arranged in a graph, linking images with driving behavior through logical reasoning. In comparison to existing benchmarks, they provide significantly more text annotations per frame (Table 1 and Fig. 2). Furthermore, as an integral component of DriveLM-CARLA, we build PDM-Lite [4], the first working rule-based expert algorithm for CARLA Leaderboard 2.0. We pair these training datasets with challenging test data for evaluating zero-shot generalization.</p>
<p><strong>æ•°æ®é›†ï¼šDriveLM-nuSceneså’ŒDriveLM-CARLA</strong><br>
è¿™ä¸¤ä¸ªæ•°æ®é›†åŒ…å«ä»¥å›¾ç»“æ„æ’åˆ—çš„å¸¦æ³¨é‡Šé—®ç­”å¯¹ï¼ˆQAsï¼‰ï¼Œé€šè¿‡é€»è¾‘æ¨ç†å°†å›¾åƒä¸é©¾é©¶è¡Œä¸ºå…³è”èµ·æ¥ã€‚ä¸ç°æœ‰åŸºå‡†æ•°æ®é›†ç›¸æ¯”ï¼Œå®ƒä»¬æ¯å¸§æä¾›çš„æ–‡æœ¬æ³¨é‡Šæ˜¾è‘—æ›´å¤šï¼ˆè¡¨1å’Œå›¾2ï¼‰ã€‚æ­¤å¤–ï¼Œä½œä¸ºDriveLM-CARLAçš„ç»„æˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬æ„å»ºäº†PDM-Lite[4]â€”â€”é¦–ä¸ªé€‚ç”¨äºCARLAæ’è¡Œæ¦œ2.0çš„åŸºäºè§„åˆ™çš„ä¸“å®¶ç®—æ³•ã€‚æˆ‘ä»¬å°†è¿™äº›è®­ç»ƒæ•°æ®é›†ä¸å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•æ•°æ®ç»“åˆï¼Œç”¨äºè¯„ä¼°é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Baseline. DriveLM-Agent</strong> employs a trajectory tokenizer that can be applied to any general VLM [47,54,65,106], coupled with a graph prompting scheme that models logical dependencies as context inputs for VLMs. The result is a simple methodology to effectively repurpose VLMs for end-to-end AD (Section 3).</p>
<p><strong>åŸºçº¿æ¨¡å‹ï¼šDriveLM-Agent</strong><br>
DriveLM-Agenté‡‡ç”¨äº†å¯åº”ç”¨äºä»»æ„é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰[47,54,65,106]çš„è½¨è¿¹æ ‡è®°å™¨ï¼Œå¹¶ç»“åˆå›¾æç¤ºæ–¹æ¡ˆâ€”â€”è¯¥æ–¹æ¡ˆå°†é€»è¾‘ä¾èµ–å…³ç³»å»ºæ¨¡ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡è¾“å…¥ã€‚æœ€ç»ˆå½¢æˆäº†ä¸€ç§å°†è§†è§‰è¯­è¨€æ¨¡å‹é«˜æ•ˆé€‚é…äºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä»»åŠ¡çš„ç®€å•æ–¹æ³•ï¼ˆç¬¬3èŠ‚ï¼‰ã€‚</p>
<p>Our experiments provide encouraging results. We find that GVQA on DriveLM is a challenging task, where current methods obtain moderate scores and better modeling of logical dependencies is likely necessary to achieve strong QA performance. Even so, DriveLM-Agent already performs competitively to state-of-the-art driving-specific models [32] when tested in the open-loop planning setting, despite its task-agnostic architecture. Furthermore, employing a graph structure improves zero-shot generalization, enabling DriveLM-Agent to correctly handle unseen deployment on Waymo data [77] after training only on nuScenes [6] data. Finally, with a question-wise analysis, we find that the QA pairs from the prediction and planning stages help the final driving decision most. From these results, we believe that improving GVQA holds great potential towards building autonomous driving agents with strong generalization.</p>
<p>æˆ‘ä»¬çš„å®éªŒæä¾›äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨DriveLMæ•°æ®é›†ä¸Šè¿›è¡Œå›¾è§†è§‰é—®ç­”ï¼ˆGVQAï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå½“å‰æ–¹æ³•ä»…è·å¾—ä¸­ç­‰åˆ†æ•°ï¼Œè€Œè‹¥è¦å®ç°å¼ºå¤§çš„é—®ç­”æ€§èƒ½ï¼Œå¯èƒ½éœ€è¦å¯¹é€»è¾‘ä¾èµ–å…³ç³»è¿›è¡Œæ›´ä¼˜çš„å»ºæ¨¡ã€‚å³ä¾¿å¦‚æ­¤ï¼Œå°½ç®¡DriveLM-Agenté‡‡ç”¨ä¸ä»»åŠ¡æ— å…³çš„æ¶æ„ï¼Œä½†åœ¨å¼€ç¯è§„åˆ’åœºæ™¯ä¸­æµ‹è¯•æ—¶ï¼Œå…¶æ€§èƒ½å·²å¯ä¸æœ€å…ˆè¿›çš„é©¾é©¶ä¸“ç”¨æ¨¡å‹[32]ç›¸åª²ç¾ã€‚æ­¤å¤–ï¼Œå›¾ç»“æ„çš„ä½¿ç”¨æå‡äº†é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å¾—DriveLM-Agentä»…åœ¨nuScenes[6]æ•°æ®ä¸Šè®­ç»ƒåï¼Œå³å¯æ­£ç¡®å¤„ç†åœ¨Waymoæ•°æ®[77]ä¸Šçš„æœªè§éƒ¨ç½²åœºæ™¯ã€‚æœ€åï¼Œé€šè¿‡é€é¡¹åˆ†ææˆ‘ä»¬å‘ç°ï¼Œæ¥è‡ªé¢„æµ‹å’Œè§„åˆ’é˜¶æ®µçš„é—®ç­”å¯¹ï¼ˆQA pairsï¼‰å¯¹æœ€ç»ˆé©¾é©¶å†³ç­–çš„å¸®åŠ©æœ€å¤§ã€‚åŸºäºè¿™äº›ç»“æœï¼Œæˆ‘ä»¬è®¤ä¸ºæ”¹è¿›å›¾è§†è§‰é—®ç­”ï¼ˆGVQAï¼‰åœ¨æ„å»ºå…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p>2 DriveLM:Task, Data, Metrics
Human drivers usually decompose their decision-making process into distinct stages that follow a logical progression, which encompasses the identification and localization of key objects, their possible future actions and interactions, and ego planning based on all this information [27,55]. This inspires us to propose the GVQA as the critical ingredient of DriveLM, which serves as a suitable proxy task to mimic the human reasoning process. Within this section, we illustrate the formulation of the GVQA task (Section 2.1), introduce DriveLM-Data (Section 2.2) to exemplify the instantiation of GVQA using prominent driving datasets, and overview the DriveLM-Metrics used for evaluation (Section 2.3). To encourage further research in this direction, an official evaluation server (with a public leaderboard) will be set up to benchmark different methods and discover more insights about combining language models with autonomous driving.</p>
<p>äººç±»é©¾é©¶å‘˜é€šå¸¸ä¼šå°†å†³ç­–è¿‡ç¨‹åˆ†è§£ä¸ºéµå¾ªé€»è¾‘é€’è¿›çš„ä¸åŒé˜¶æ®µï¼ŒåŒ…æ‹¬å…³é”®ç‰©ä½“çš„è¯†åˆ«ä¸å®šä½ã€å…¶æœªæ¥å¯èƒ½çš„åŠ¨ä½œåŠäº¤äº’ï¼Œä»¥åŠåŸºäºæ‰€æœ‰è¿™äº›ä¿¡æ¯çš„è‡ªè½¦è§„åˆ’[27,55]ã€‚è¿™å¯å‘æˆ‘ä»¬å°†å›¾è§†è§‰é—®ç­”ï¼ˆGVQAï¼‰ä½œä¸ºDriveLMçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œä½¿å…¶æˆä¸ºæ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹çš„åˆé€‚ä»£ç†ä»»åŠ¡ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é˜è¿°GVQAä»»åŠ¡çš„å½¢å¼åŒ–å®šä¹‰ï¼ˆç¬¬2.1èŠ‚ï¼‰ï¼Œä»‹ç»DriveLM-Dataæ•°æ®é›†ï¼ˆç¬¬2.2èŠ‚ï¼‰ä»¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ä¸»æµé©¾é©¶æ•°æ®é›†å®ç°GVQAä»»åŠ¡å®ä¾‹åŒ–ï¼Œå¹¶æ¦‚è¿°ç”¨äºè¯„ä¼°çš„DriveLMæŒ‡æ ‡ï¼ˆç¬¬2.3èŠ‚ï¼‰ã€‚ä¸ºæ¨åŠ¨è¯¥æ–¹å‘çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†æ­å»ºå®˜æ–¹è¯„ä¼°æœåŠ¡å™¨ï¼ˆå«å…¬å¼€æ’è¡Œæ¦œï¼‰ï¼Œä»¥å¯¹ä¸åŒæ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æ¢ç´¢è¯­è¨€æ¨¡å‹ä¸è‡ªåŠ¨é©¾é©¶ç»“åˆçš„æ›´å¤šæ´è§ã€‚</p>
<p>2.1 DriveLM-Task:GVQA</p>
<p>We organize all the question-answer pairs (QAs) for an image frame into a graph. We use the terminology &ldquo;graph-structured&rdquo; to refer to a directed acyclic graph (DAG), e.g., the current question can get context from (multiple) parent and grandparent nodes. The graph $G = (V, E)$ contains a set of vertices $V$, where each vertex represents a QA pair $v = (q, a)$ associated with one or more key objects in the scenario. The key difference between GVQA and ordinary VQA is that the QAs in GVQA have logical dependencies, which we formulate as the edges between the vertices. $E \subseteq V Ã— V$ is a set of directed edges, where each edge $e = (v_p, v_c)$ connects the parent QA and the child QA. We formulate the edge set $E$ by incorporating two dimensions: object-level and task-level edges. At the object level, we construct the logical edges $e \in E$ to represent the impact of interactions between different objects. For example, the planning QA node for the sedan is influenced by the perception QA node of the pedestrian in the illustration from Fig. 1 (center). At the task level, we establish the logical edges $e \in E$ to capture the logical chain of different reasoning stages:</p>
<p>æˆ‘ä»¬å°†å›¾åƒå¸§çš„æ‰€æœ‰é—®ç­”å¯¹ï¼ˆQAsï¼‰ç»„ç»‡æˆå›¾ç»“æ„ã€‚æ­¤å¤„â€œå›¾ç»“æ„â€æŒ‡æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œä¾‹å¦‚å½“å‰é—®é¢˜å¯ä»ï¼ˆå¤šä¸ªï¼‰çˆ¶èŠ‚ç‚¹å’Œç¥–çˆ¶èŠ‚ç‚¹è·å–ä¸Šä¸‹æ–‡ã€‚å›¾ $G = (V, E)$ åŒ…å«ä¸€ç»„é¡¶ç‚¹ $V$ï¼Œæ¯ä¸ªé¡¶ç‚¹è¡¨ç¤ºä¸€ä¸ªé—®ç­”å¯¹ $v = (q, a)$ï¼Œè¯¥é—®ç­”å¯¹ä¸åœºæ™¯ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªå…³é”®ç‰©ä½“ç›¸å…³è”ã€‚GVQA ä¸æ™®é€š VQA çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼ŒGVQA ä¸­çš„é—®ç­”å¯¹å…·æœ‰é€»è¾‘ä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬å°†å…¶è¡¨ç¤ºä¸ºé¡¶ç‚¹ä¹‹é—´çš„è¾¹ã€‚$E \subseteq V Ã— V$ æ˜¯ä¸€ç»„æœ‰å‘è¾¹ï¼Œæ¯æ¡è¾¹ $e = (v_p, v_c)$ è¿æ¥çˆ¶é—®ç­”å¯¹å’Œå­é—®ç­”å¯¹ã€‚æˆ‘ä»¬é€šè¿‡èåˆä¸¤ä¸ªç»´åº¦æ¥æ„å»ºè¾¹é›† $E$ï¼š<strong>ç‰©ä½“çº§è¾¹</strong>å’Œ<strong>ä»»åŠ¡çº§è¾¹</strong>ã€‚</p>
<ul>
<li><strong>ç‰©ä½“çº§</strong>ï¼šæˆ‘ä»¬æ„å»ºé€»è¾‘è¾¹ $e \in E$ ä»¥è¡¨ç¤ºä¸åŒç‰©ä½“é—´äº¤äº’çš„å½±å“ã€‚ä¾‹å¦‚ï¼Œå›¾1ï¼ˆä¸­å¿ƒï¼‰ç¤ºä¾‹ä¸­ï¼Œè½¿è½¦çš„è§„åˆ’é—®ç­”èŠ‚ç‚¹ä¼šå—åˆ°è¡Œäººçš„æ„ŸçŸ¥é—®ç­”èŠ‚ç‚¹çš„å½±å“ã€‚</li>
<li><strong>ä»»åŠ¡çº§</strong>ï¼šæˆ‘ä»¬å»ºç«‹é€»è¾‘è¾¹ $e \in E$ ä»¥æ•æ‰ä¸åŒæ¨ç†é˜¶æ®µçš„é€»è¾‘é“¾æ¡ï¼š</li>
<li>æ„ŸçŸ¥$(P_1)$ï¼šè¯†åˆ«ã€æè¿°å¹¶å®šä½å½“å‰åœºæ™¯ä¸­çš„å…³é”®ç‰©ä½“ã€‚</li>
<li>é¢„æµ‹$(P_2)$ï¼šåŸºäºæ„ŸçŸ¥ç»“æœï¼Œä¼°è®¡å…³é”®ç‰©ä½“å¯èƒ½çš„åŠ¨ä½œæˆ–äº¤äº’ã€‚</li>
<li>è§„åˆ’$(P_3)$ï¼šè‡ªè½¦å¯èƒ½é‡‡å–çš„å®‰å…¨åŠ¨ä½œã€‚</li>
<li>è¡Œä¸º$(B)$ï¼šå¯¹é©¾é©¶å†³ç­–è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>è¿åŠ¨$(M)$ï¼šè‡ªè½¦æœªæ¥è½¨è¿¹çš„è·¯å¾„ç‚¹ã€‚</li>
</ul>
<p>æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ $P_{1-3})$ çš„æ¦‚å¿µä¸ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¦‚å¿µç±»ä¼¼[10]ï¼Œè€Œè¿åŠ¨å’Œè¡Œä¸ºçš„æ¦‚å¿µåˆ™åŸºäºè‡ªè½¦æœªæ¥è½¨è¿¹ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†è¿åŠ¨ $M$ å®šä¹‰ä¸ºè‡ªè½¦æœªæ¥è½¨è¿¹ï¼Œå®ƒæ˜¯é¸Ÿç°å›¾ï¼ˆBEVï¼‰ä¸­ä¸€ç»„åæ ‡ä¸º $(x, y)$ çš„ $N$ ä¸ªç‚¹ï¼Œè®°ä¸º $M={(x_0,y_0),(x_1,y_1),...,(x_N,y_N)}$ã€‚æ¯ä¸ªç‚¹æ˜¯æœªæ¥ä½ç½®ä¸å½“å‰ä½ç½®åœ¨å›ºå®šæ—¶é—´é—´éš”ä¸‹çš„åç§»é‡ã€‚ç„¶åï¼Œæ¯ä¸ªæ—¶é—´é—´éš”å†… $x, y$ çš„è·ç¦»è®¡ç®—ä¸ºï¼š</p>
<p>å…¶ä¸­ï¼Œå¯¹äº$i = 1, 2, \dots, N$ï¼Œ$\delta x_i = x_i - x_{i-1}$ä¸”$\delta y_i = y_i - y_{i-1}$ã€‚è¡Œä¸ºè¡¨å¾çš„ç›®æ ‡æ˜¯ä½œä¸ºè¿æ¥P1-3ï¼ˆæ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’ï¼‰ä¸è¿åŠ¨Mçš„æ¥å£ã€‚ä¸ºäº†è·å¾—è¡Œä¸ºè¡¨å¾ï¼Œæˆ‘ä»¬å°†$x$æ–¹å‘è·ç¦»å˜åŒ–ï¼ˆ$x_{\text{dist}}$ï¼‰å’Œ$y$æ–¹å‘è·ç¦»å˜åŒ–ï¼ˆ$y_{\text{dist}}$ï¼‰çš„å‡å€¼æ˜ å°„åˆ°é¢„å®šä¹‰çš„åŒºé—´ä¹‹ä¸€ï¼Œæ¯ä¸ªåŒºé—´å¯¹åº”é€Ÿåº¦æˆ–è½¬å‘ä¸­çš„ä¸€ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«è®°ä¸º$B_{\text{sp}}$å’Œ$B_{\text{st}}$ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘5ä¸ªåŒºé—´ï¼š</p>

        
      </section>

      

      
    </article>
  </div>

  <div
    x-data="tocHighlighter()"
    @scroll.window="debouncedScroll"
    class="hidden lg:flex lg:flex-col lg:items-end">
    
      <nav id="TableOfContents"></nav>
    
  </div>
</div>


            
<footer class="flex justify-between items-center gap-2 max-w-[65ch] mx-auto px-4 md:px-0 py-12">

  <div>
  
  <p>
    Â© 2025 dO.ob&#39;s Blog
  </p>
  

  
  <p class="text-sm">
    ğŸŒ±
    <span class="text-base-content/60">
      Powered by <a class="hover:underline" href="https://gohugo.io/" target="_blank">Hugo</a> with theme
      <a class="hover:underline" href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</span
    >
  </p>
  
</div>

  <div
  x-data="{ icons: [
    { name: 'sunny', status: 'n' },
    { name: 'moon', status: 'y' },
    { name: 'desktop', status: 'auto' }
  ] }"
  class="flex items-center gap-2 h-[32px] px-2 bg-base-100 border border-base-content/30 rounded-full"
>
  <template x-for="icon in icons">
    <div
      role="button"
      tabindex="0"
      :aria-label="'Select ' + icon.name + ' mode'"
      class="group inline-flex justify-center items-center p-1 rounded-full cursor-pointer hover:bg-primary"
      :class="$store.darkMode.icon() === icon.name && 'bg-primary'"
      @click="$store.darkMode.toggle(icon.status)"
    >
      <ion-icon
        :name="`${icon.name}-outline`"
        class="group-hover:text-primary-content"
        :class="$store.darkMode.icon() === icon.name && 'text-primary-content'"
      >
      </ion-icon>
    </div>
  </template>
</div>

</footer>

          </div>
        </div>
        <div class="back">
          <div class="container">
            
            <div class="max-w-[65ch] mt-8 mx-auto px-4">
  
  
  
  <div>
    <div class="mb-4 text-lg font-medium">å…³äºæˆ‘</div>

    <div class="prose dark:prose-invert">
      <p>æˆ‘æ˜¯ä¸€åAIä»ä¸šè€…ï¼Œè‡´åŠ›äºäººå·¥æ™ºèƒ½å’Œæ•°æ®é©±åŠ¨è¡Œä¸šã€‚</p>
<p>è¸ä¸Šå–ç»è·¯ï¼Œæ¯”æŠµè¾¾çµå±±æ›´é‡è¦</p>

    </div>
  </div>
  <div class="divider divider-vertical"></div>
  
  

  

  
</div>

            

            
<footer class="flex justify-between items-center gap-2 max-w-[65ch] mx-auto px-4 md:px-0 py-12">

  <div>
  
  <p>
    Â© 2025 dO.ob&#39;s Blog
  </p>
  

  
  <p class="text-sm">
    ğŸŒ±
    <span class="text-base-content/60">
      Powered by <a class="hover:underline" href="https://gohugo.io/" target="_blank">Hugo</a> with theme
      <a class="hover:underline" href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</span
    >
  </p>
  
</div>

  <div
  x-data="{ icons: [
    { name: 'sunny', status: 'n' },
    { name: 'moon', status: 'y' },
    { name: 'desktop', status: 'auto' }
  ] }"
  class="flex items-center gap-2 h-[32px] px-2 bg-base-100 border border-base-content/30 rounded-full"
>
  <template x-for="icon in icons">
    <div
      role="button"
      tabindex="0"
      :aria-label="'Select ' + icon.name + ' mode'"
      class="group inline-flex justify-center items-center p-1 rounded-full cursor-pointer hover:bg-primary"
      :class="$store.darkMode.icon() === icon.name && 'bg-primary'"
      @click="$store.darkMode.toggle(icon.status)"
    >
      <ion-icon
        :name="`${icon.name}-outline`"
        class="group-hover:text-primary-content"
        :class="$store.darkMode.icon() === icon.name && 'text-primary-content'"
      >
      </ion-icon>
    </div>
  </template>
</div>

</footer>

          </div>
        </div>
      </div>
    </div>

    <script>
  window.lightTheme = "emerald"
  window.darkTheme = "forest"
</script>





<script src="/js/main.js"></script>

    







<script src="/js/toc.js"></script>





    

    

    

    

    <script type="module" src="https://cdn.jsdelivr.net/npm/ionicons@7.4.0/dist/ionicons/ionicons.esm.js" integrity="sha256-/IFmi82bIhdYWctu0UddSlJqpnzWm7Vh2C4CM32wF/k=" crossorigin="anonymous"></script>
    <script nomodule src="https://cdn.jsdelivr.net/npm/ionicons@7.4.0/dist/ionicons/ionicons.js" integrity="sha256-mr7eJMX3VC3F7G32mk4oWp1C6a2tlMYxUdptfT7uKI8=" crossorigin="anonymous"></script>
  </body>
</html>
